{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChainä»å…¥é—¨åˆ°å®è·µï¼šæ•°æ®è¿æ¥ä¸æµç¨‹ç¼–æ’\n",
    "\n",
    "åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LangChainçš„åŸºç¡€ç»„ä»¶ï¼ŒåŒ…æ‹¬æ¨¡å‹å°è£…ã€Promptæ¨¡æ¿ã€ç»“æ„åŒ–è¾“å‡ºå’ŒFunction Callingã€‚æœ¬ç¯‡æ–‡ç« å°†ç»§ç»­æ·±å…¥ï¼Œä¸ºå¤§å®¶ä»‹ç»LangChainçš„æ•°æ®è¿æ¥èƒ½åŠ›å’Œæµç¨‹ç¼–æ’åŠŸèƒ½ï¼Œå¸®åŠ©ä½ æ„å»ºæ›´å¤æ‚çš„å¤§æ¨¡å‹åº”ç”¨ã€‚\n",
    "\n",
    "æ³¨æ„äº‹é¡¹ï¼š\n",
    "è¿è¡Œæœ¬æ–‡ä»£ç å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…ï¼špip install langchain-openai langchain-text-splitters langchain-community faiss-cpu pymupdf langchain-dashscope\n",
    "è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®ç›¸åº”çš„APIå¯†é’¥\n",
    "ç¤ºä¾‹ä¸­çš„æ–‡ä»¶è·¯å¾„éœ€è¦æ›¿æ¢ä¸ºä½ è‡ªå·±çš„æ–‡ä»¶è·¯å¾„\n",
    "\n",
    "## 1. æ•°æ®è¿æ¥å°è£…\n",
    "\n",
    "å¤§æ¨¡å‹åº”ç”¨é€šå¸¸éœ€è¦è¿æ¥å¤–éƒ¨æ•°æ®æºï¼ŒLangChainæä¾›äº†ä¸°å¯Œçš„æ•°æ®åŠ è½½å’Œå¤„ç†ç»„ä»¶ï¼Œè®©æˆ‘ä»¬å¯ä»¥è½»æ¾å¤„ç†å„ç§æ ¼å¼çš„æ–‡æ¡£ã€‚\n",
    "\n",
    "### 1.1 æ–‡æ¡£åŠ è½½å™¨ï¼šDocument Loaders\n",
    "\n",
    "æ–‡æ¡£åŠ è½½å™¨è´Ÿè´£ä»ä¸åŒæ¥æºåŠ è½½æ–‡æ¡£ï¼Œä»¥ä¸‹æ˜¯ä½¿ç”¨PyMuPDFåŠ è½½PDFæ–‡æ¡£çš„ç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvronâˆ—\n",
      "Louis Martinâ€ \n",
      "Kevin Stoneâ€ \n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov\n",
      "Thomas Scialomâˆ—\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ï¬ne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "source models. We provide a detailed description of our approach to ï¬ne-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "â€ Second author\n",
      "Contributions for all the authors can be found in Section A.1.\n",
      "arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£…å¿…è¦çš„ä¾èµ–\n",
    "# %pip install pymupdf langchain-openai langchain-text-splitters faiss-cpu\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# åŠ è½½PDFæ–‡æ¡£\n",
    "loader = PyMuPDFLoader(\"./data/rag_data/llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# æ‰“å°ç¬¬ä¸€é¡µå†…å®¹\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChainæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼ŒåŒ…æ‹¬PDFã€Wordã€CSVã€HTMLç­‰ï¼Œä½ å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©åˆé€‚çš„åŠ è½½å™¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æ–‡æœ¬åˆ†å‰²å™¨\n",
    "\n",
    "æ–‡æœ¬åˆ†å‰²å™¨ç”¨äºå°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆé€‚åˆå¤§æ¨¡å‹å¤„ç†çš„å°å—ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvronâˆ—\n",
      "Louis Martinâ€ \n",
      "Kevin Stoneâ€ \n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "-------\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "-------\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "-------\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "-------\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "-------\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov\n",
      "Thomas Scialomâˆ—\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "-------\n",
      "Sergey Edunov\n",
      "Thomas Scialomâˆ—\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "-------\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ï¬ne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "-------\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "source models. We provide a detailed description of our approach to ï¬ne-tuning and safety\n",
      "-------\n",
      "source models. We provide a detailed description of our approach to ï¬ne-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "-------\n",
      "contribute to the responsible development of LLMs.\n",
      "âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "â€ Second author\n",
      "Contributions for all the authors can be found in Section A.1.\n",
      "arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,         # æ¯ä¸ªæ–‡æœ¬å—çš„ç›®æ ‡å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰\n",
    "    chunk_overlap=100,      # ç›¸é‚»æ–‡æœ¬å—ä¹‹é—´çš„é‡å éƒ¨åˆ†å¤§å°\n",
    "    length_function=len,    # ç”¨äºè®¡ç®—æ–‡æœ¬é•¿åº¦çš„å‡½æ•°\n",
    "    add_start_index=True,   # æ˜¯å¦æ·»åŠ èµ·å§‹ç´¢å¼•ä¿¡æ¯\n",
    ")\n",
    "\n",
    "# åˆ†å‰²æ–‡æœ¬\n",
    "paragraphs = text_splitter.create_documents([pages[0].page_content])\n",
    "for para in paragraphs:\n",
    "    print(para.page_content)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 å‘é‡æ•°æ®åº“ä¸å‘é‡æ£€ç´¢\n",
    "\n",
    "LangChainæä¾›äº†ä¸å¤šç§å‘é‡æ•°æ®åº“çš„é›†æˆï¼Œæ–¹ä¾¿å®ç°è¯­ä¹‰æœç´¢ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but are not releasing.Â§\n",
      "2. Llama 2-Chat, a ï¬ne-tuned version of Llama 2 that is optimized for dialogue use cases. We release\n",
      "variants of this model with 7B, 13B, and 70B parameters as well.\n",
      "We believe that the open release of LLMs, when done safely, will be a net beneï¬t to society. Like all LLMs,\n",
      "----\n",
      "Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "Llama 2-Chat models generally perform better than existing open-source models. They also appear to\n",
      "----\n",
      "Sergey Edunov\n",
      "Thomas Scialomâˆ—\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "loader = PyMuPDFLoader(\"./data/rag_data/llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# æ–‡æ¡£åˆ‡åˆ†\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:4]]\n",
    ")\n",
    "\n",
    "# åˆ›å»ºå‘é‡æ•°æ®åº“\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# æ£€ç´¢ç›¸å…³æ–‡æ¡£\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})  # æ£€ç´¢top-3ç›¸å…³æ–‡æ¡£\n",
    "query = \"llama2æœ‰å¤šå°‘å‚æ•°?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# æ‰“å°æ£€ç´¢ç»“æœ\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chainå’ŒLangChain Expression Language (LCEL)\n",
    "\n",
    "Chainæ˜¯LangChainçš„æ ¸å¿ƒæ¦‚å¿µä¹‹ä¸€ï¼Œå®ƒå…è®¸æˆ‘ä»¬å°†å¤šä¸ªç»„ä»¶è¿æ¥èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„å¤„ç†æµç¨‹ã€‚LangChain Expression Language (LCEL)æ˜¯LangChainæä¾›çš„ä¸€ç§å£°æ˜å¼è¯­è¨€ï¼Œç”¨äºç®€åŒ–Chainçš„æ„å»ºè¿‡ç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ä½¿ç”¨LCELæ„å»ºç®€å•Pipeline\n",
    "\n",
    "LCELå…è®¸æˆ‘ä»¬ä½¿ç”¨ç®¡é“ç¬¦|å°†ä¸åŒç»„ä»¶è¿æ¥èµ·æ¥ï¼Œå½¢æˆå¤„ç†æµç¨‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": null,\n",
      "    \"price_lower\": null,\n",
      "    \"price_upper\": 100,\n",
      "    \"data_lower\": null,\n",
      "    \"data_upper\": null,\n",
      "    \"sort_by\": \"data\",\n",
      "    \"ordering\": \"descend\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "import json\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# å®šä¹‰è¾“å‡ºç»“æ„\n",
    "class SortEnum(str, Enum):\n",
    "    data = 'data'   # æŒ‰æµé‡æ’åº\n",
    "    price = 'price' # æŒ‰ä»·æ ¼æ’åº\n",
    "\n",
    "class OrderingEnum(str, Enum):\n",
    "    ascend = 'ascend'   # å‡åº\n",
    "    descend = 'descend' # é™åº\n",
    "\n",
    "class Semantics(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"æµé‡åŒ…åç§°\", default=None)\n",
    "    price_lower: Optional[int] = Field(description=\"ä»·æ ¼ä¸‹é™\", default=None)\n",
    "    price_upper: Optional[int] = Field(description=\"ä»·æ ¼ä¸Šé™\", default=None)\n",
    "    data_lower: Optional[int] = Field(description=\"æµé‡ä¸‹é™\", default=None)\n",
    "    data_upper: Optional[int] = Field(description=\"æµé‡ä¸Šé™\", default=None)\n",
    "    sort_by: Optional[SortEnum] = Field(description=\"æŒ‰ä»·æ ¼æˆ–æµé‡æ’åº\", default=None)\n",
    "    ordering: Optional[OrderingEnum] = Field(description=\"å‡åºæˆ–é™åºæ’åˆ—\", default=None)\n",
    "\n",
    "# Promptæ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªè¯­ä¹‰è§£æå™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„è¾“å…¥è§£ææˆJSONè¡¨ç¤ºã€‚ä¸è¦å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\"),\n",
    "    (\"human\", \"{text}\"),\n",
    "])\n",
    "\n",
    "# æ¨¡å‹\n",
    "llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "structured_llm = llm.with_structured_output(Semantics)\n",
    "\n",
    "# LCELè¡¨è¾¾å¼\n",
    "runnable = (\n",
    "    {\"text\": RunnablePassthrough()} | prompt | structured_llm\n",
    ")\n",
    "\n",
    "# è¿è¡Œ\n",
    "ret = runnable.invoke(\"ä¸è¶…è¿‡100å…ƒçš„æµé‡å¤§çš„å¥—é¤æœ‰å“ªäº›\")\n",
    "print(json.dumps(ret.model_dump(), indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ä½¿ç”¨LCELå®ç°RAG\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) æ˜¯å¤§æ¨¡å‹åº”ç”¨çš„ä¸€ç§é‡è¦æ¨¡å¼ï¼ŒLCELå¯ä»¥å¸®åŠ©æˆ‘ä»¬è½»æ¾å®ç°ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ¹æ®æä¾›çš„å†…å®¹ï¼Œæ–‡æ¡£ä¸­åˆ—å‡ºäº†ä»¥ä¸‹éƒ¨åˆ†çš„é‡è¦ä¿¡æ¯ï¼š\n",
      "\n",
      "1. **æ•°æ®é¢„è®­ç»ƒï¼ˆPretrainingï¼‰**ï¼š\n",
      "   - å‰é¦ˆæ•°æ®ï¼ˆPretraining Dataï¼‰\n",
      "   - è®­ç»ƒç»†èŠ‚ï¼ˆTraining Detailsï¼‰\n",
      "\n",
      "2. **é™„å½•éƒ¨åˆ†**ï¼š\n",
      "   - æ•°æ®æ ‡æ³¨ï¼ˆData Annotationï¼‰\n",
      "   - æ•°æ®é›†æ±¡æŸ“ï¼ˆDataset Contaminationï¼‰\n",
      "   - æ¨¡å‹å¡ï¼ˆModel Cardï¼‰\n",
      "\n",
      "è¿™äº›éƒ¨åˆ†æ¶‰åŠåˆ°æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹åŠç›¸å…³æ•°æ®çš„ä¿¡æ¯ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "loader = PyMuPDFLoader(\"./data/rag_data/llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# æ–‡æ¡£åˆ‡åˆ†\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:4]]\n",
    ")\n",
    "\n",
    "# åˆ›å»ºå‘é‡æ•°æ®åº“\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Promptæ¨¡æ¿\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# æ„å»ºRAG Chain\n",
    "rag_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# è¿è¡ŒRAG\n",
    "answer = rag_chain.invoke(\"æ–‡æ¡£ä¸­æœ‰ä»€ä¹ˆé‡è¦ä¿¡æ¯\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ä½¿ç”¨LCELå®ç°å·¥å‚æ¨¡å¼\n",
    "\n",
    "LCELè¿˜æ”¯æŒå·¥å‚æ¨¡å¼ï¼Œå¯ä»¥æ ¹æ®éœ€è¦åŠ¨æ€é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-dashscope\n",
      "  Downloading langchain_dashscope-0.1.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: dashscope<2.0.0,>=1.14.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-dashscope) (1.22.2)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from langchain-dashscope)\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from dashscope<2.0.0,>=1.14.1->langchain-dashscope) (3.11.12)\n",
      "Requirement already satisfied: requests in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from dashscope<2.0.0,>=1.14.1->langchain-dashscope) (2.32.3)\n",
      "Requirement already satisfied: websocket-client in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from dashscope<2.0.0,>=1.14.1->langchain-dashscope) (1.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from tiktoken<0.8.0,>=0.7.0->langchain-dashscope) (2024.5.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (4.12.2)\n",
      "Downloading langchain_dashscope-0.1.8-py3-none-any.whl (9.0 kB)\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-win_amd64.whl (798 kB)\n",
      "   ---------------------------------------- 0.0/798.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/798.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/798.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 20.5/798.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 20.5/798.9 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/798.9 kB 262.6 kB/s eta 0:00:03\n",
      "   - ------------------------------------- 30.7/798.9 kB 262.6 kB/s eta 0:00:03\n",
      "   - ------------------------------------- 41.0/798.9 kB 178.6 kB/s eta 0:00:05\n",
      "   - ------------------------------------- 41.0/798.9 kB 178.6 kB/s eta 0:00:05\n",
      "   -- ------------------------------------ 61.4/798.9 kB 218.8 kB/s eta 0:00:04\n",
      "   -- ------------------------------------ 61.4/798.9 kB 218.8 kB/s eta 0:00:04\n",
      "   --- ----------------------------------- 81.9/798.9 kB 229.0 kB/s eta 0:00:04\n",
      "   ----- -------------------------------- 112.6/798.9 kB 297.7 kB/s eta 0:00:03\n",
      "   ------ ------------------------------- 143.4/798.9 kB 327.9 kB/s eta 0:00:02\n",
      "   ------- ------------------------------ 163.8/798.9 kB 350.7 kB/s eta 0:00:02\n",
      "   --------- ---------------------------- 194.6/798.9 kB 368.6 kB/s eta 0:00:02\n",
      "   ----------- -------------------------- 245.8/798.9 kB 430.6 kB/s eta 0:00:02\n",
      "   ------------ ------------------------- 256.0/798.9 kB 436.5 kB/s eta 0:00:02\n",
      "   ------------- ------------------------ 276.5/798.9 kB 448.2 kB/s eta 0:00:02\n",
      "   ---------------- --------------------- 337.9/798.9 kB 499.5 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 389.1/798.9 kB 550.7 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 491.5/798.9 kB 641.7 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 532.5/798.9 kB 668.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 583.7/798.9 kB 692.6 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 686.1/798.9 kB 786.8 kB/s eta 0:00:01\n",
      "   -------------------------------------  778.2/798.9 kB 847.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 798.9/798.9 kB 840.8 kB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken, langchain-dashscope\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.9.0\n",
      "    Uninstalling tiktoken-0.9.0:\n",
      "      Successfully uninstalled tiktoken-0.9.0\n",
      "Successfully installed langchain-dashscope-0.1.8 tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\Anacanda3\\envs\\pytorch_cuda12_0_py310\\Lib\\site-packages\\~iktoken'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-dashscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qianfan\n",
      "  Downloading qianfan-0.4.12.3-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: aiohttp>=3.7.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (3.11.12)\n",
      "Collecting aiolimiter>=1.1.0 (from qianfan)\n",
      "  Downloading aiolimiter-1.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: bce-python-sdk>=0.8.79 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (0.9.29)\n",
      "Requirement already satisfied: cachetools>=5.0.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (5.5.2)\n",
      "Collecting diskcache>=5.6.3 (from qianfan)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting multiprocess>=0.70.12 (from qianfan)\n",
      "  Downloading multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.38 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (3.0.47)\n",
      "Requirement already satisfied: pydantic>=1.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (2.10.2)\n",
      "Requirement already satisfied: python-dotenv>=1.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (1.0.1)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.24 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (2.32.3)\n",
      "Requirement already satisfied: rich>=13.0.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (13.9.4)\n",
      "Collecting tenacity<9.0.0,>=8.2.3 (from qianfan)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (0.15.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (1.18.3)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from bce-python-sdk>=0.8.79->qianfan) (3.22.0)\n",
      "Requirement already satisfied: future>=0.6.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from bce-python-sdk>=0.8.79->qianfan) (1.0.0)\n",
      "Requirement already satisfied: six>=1.4.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from bce-python-sdk>=0.8.79->qianfan) (1.16.0)\n",
      "Collecting dill>=0.4.0 (from multiprocess>=0.70.12->qianfan)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: wcwidth in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from prompt-toolkit>=3.0.38->qianfan) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from pydantic>=1.0->qianfan) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from pydantic>=1.0->qianfan) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from pydantic>=1.0->qianfan) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests>=2.24->qianfan) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests>=2.24->qianfan) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests>=2.24->qianfan) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests>=2.24->qianfan) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from rich>=13.0.0->qianfan) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from rich>=13.0.0->qianfan) (2.18.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from typer>=0.9.0->qianfan) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from typer>=0.9.0->qianfan) (1.5.4)\n",
      "Requirement already satisfied: colorama in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from click>=8.0.0->typer>=0.9.0->qianfan) (0.4.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.0.0->qianfan) (0.1.2)\n",
      "Downloading qianfan-0.4.12.3-py3-none-any.whl (470 kB)\n",
      "   ---------------------------------------- 0.0/470.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/470.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/470.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/470.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/470.3 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 41.0/470.3 kB 1.9 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 41.0/470.3 kB 1.9 MB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 51.2/470.3 kB 435.7 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 71.7/470.3 kB 491.5 kB/s eta 0:00:01\n",
      "   ------ -------------------------------- 81.9/470.3 kB 381.3 kB/s eta 0:00:02\n",
      "   --------- ---------------------------- 122.9/470.3 kB 481.4 kB/s eta 0:00:01\n",
      "   ---------- --------------------------- 133.1/470.3 kB 462.0 kB/s eta 0:00:01\n",
      "   ------------ ------------------------- 153.6/470.3 kB 482.7 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 184.3/470.3 kB 506.0 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 215.0/470.3 kB 546.1 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 266.2/470.3 kB 584.5 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 327.7/470.3 kB 676.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 399.4/470.3 kB 754.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 470.3/470.3 kB 817.1 kB/s eta 0:00:00\n",
      "Downloading aiolimiter-1.2.1-py3-none-any.whl (6.7 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.5/45.5 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.18-py310-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.9 kB ? eta -:--:--\n",
      "   ---------------------------------------  133.1/134.9 kB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 134.9/134.9 kB 2.7 MB/s eta 0:00:00\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "   ---------------------------------------- 0.0/119.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 119.7/119.7 kB 7.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tenacity, diskcache, dill, aiolimiter, multiprocess, qianfan\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "Successfully installed aiolimiter-1.2.1 dill-0.4.0 diskcache-5.6.3 multiprocess-0.70.18 qianfan-0.4.12.3 tenacity-8.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install qianfan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯ **DeepSeek Chat**ï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸ï¼ˆDeepSeekï¼‰ç ”å‘çš„æ™ºèƒ½AIåŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®åŠ©ä½ è§£ç­”å„ç§é—®é¢˜ï¼ŒåŒ…æ‹¬å­¦ä¹ ã€å·¥ä½œã€ç¼–ç¨‹ã€å†™ä½œã€ç¿»è¯‘ã€ç”Ÿæ´»å»ºè®®ç­‰ã€‚  \n",
      "\n",
      "### **æˆ‘çš„ç‰¹ç‚¹**ï¼š  \n",
      "ğŸ”¹ **å…è´¹ä½¿ç”¨**ï¼šç›®å‰æ— éœ€ä»˜è´¹ï¼Œéšæ—¶ä¸ºä½ æä¾›å¸®åŠ©ï¼  \n",
      "ğŸ”¹ **è¶…é•¿ä¸Šä¸‹æ–‡**ï¼šæ”¯æŒ **128K** ä¸Šä¸‹æ–‡è®°å¿†ï¼Œèƒ½å¤„ç†è¶…é•¿æ–‡æ¡£å’Œå¤æ‚å¯¹è¯ã€‚  \n",
      "ğŸ”¹ **å¤šæ–‡æ¡£å¤„ç†**ï¼šå¯ä»¥ä¸Šä¼  **PDFã€Wordã€Excelã€PPTã€TXT** ç­‰æ–‡ä»¶ï¼Œå¹¶å¸®ä½ æå–å’Œåˆ†æå†…å®¹ã€‚  \n",
      "ğŸ”¹ **çŸ¥è¯†ä¸°å¯Œ**ï¼šæˆ‘çš„çŸ¥è¯†æˆªæ­¢åˆ° **2024å¹´7æœˆ**ï¼Œèƒ½æä¾›è¾ƒæ–°çš„ä¿¡æ¯ã€‚  \n",
      "ğŸ”¹ **å¤šè¯­è¨€æ”¯æŒ**ï¼šå¯ä»¥ç”¨ä¸­æ–‡ã€è‹±æ–‡ç­‰å¤šç§è¯­è¨€äº¤æµã€‚  \n",
      "\n",
      "### **æˆ‘èƒ½å¸®ä½ åšä»€ä¹ˆï¼Ÿ**  \n",
      "ğŸ“š **å­¦ä¹ **ï¼šè§£é¢˜æ€è·¯ã€è®ºæ–‡æ¶¦è‰²ã€çŸ¥è¯†è®²è§£  \n",
      "ğŸ’¼ **å·¥ä½œ**ï¼šç®€å†ä¼˜åŒ–ã€é‚®ä»¶æ’°å†™ã€æ•°æ®åˆ†æ  \n",
      "ğŸ’» **ç¼–ç¨‹**ï¼šä»£ç è°ƒè¯•ã€ç®—æ³•è®²è§£ã€æŠ€æœ¯æ–‡æ¡£è§£è¯»  \n",
      "âœï¸ **å†™ä½œ**ï¼šåˆ›æ„å†™ä½œã€æ–‡æ¡ˆä¼˜åŒ–ã€æ•…äº‹æ„æ€  \n",
      "ğŸŒ **ç”Ÿæ´»**ï¼šæ—…è¡Œå»ºè®®ã€å¥åº·å°è´´å£«ã€å¨±ä¹æ¨è  \n",
      "\n",
      "ä½ å¯ä»¥éšæ—¶å‘æˆ‘æé—®ï¼Œæˆ‘ä¼šå°½åŠ›æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„ç­”æ¡ˆï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.utils import ConfigurableField\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.chat_models import QianfanChatEndpoint  # ä½¿ç”¨QianfanChatEndpoint\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# åˆå§‹åŒ–ä¸åŒçš„æ¨¡å‹\n",
    "ds_model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "gpt_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "# é…ç½®å¯é€‰æ¨¡å‹\n",
    "model = gpt_model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"), \n",
    "    default_key=\"gpt\",  # è®¾ç½®é»˜è®¤æ¨¡å‹ä¸ºGPT\n",
    "    deepseek=ds_model,\n",
    "    # å¯ä»¥æ·»åŠ æ›´å¤šæ¨¡å‹\n",
    ")\n",
    "\n",
    "# Promptæ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{query}\"),\n",
    "])\n",
    "\n",
    "# æ„å»ºChain\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# è¿è¡Œæ—¶æŒ‡å®šæ¨¡å‹\n",
    "ret = chain.with_config(configurable={\"llm\": \"deepseek\"}).invoke(\"è¯·è‡ªæˆ‘ä»‹ç»\")\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€šè¿‡LCELï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å®ç°æ›´å¤šé«˜çº§åŠŸèƒ½ï¼š\n",
    "\n",
    "é…ç½®è¿è¡Œæ—¶å˜é‡\n",
    "æ•…éšœå›é€€\n",
    "å¹¶è¡Œè°ƒç”¨\n",
    "é€»è¾‘åˆ†æ”¯\n",
    "åŠ¨æ€åˆ›å»ºChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ€»ç»“\n",
    "åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LangChainçš„æ•°æ®è¿æ¥èƒ½åŠ›å’Œæµç¨‹ç¼–æ’åŠŸèƒ½ã€‚è¿™äº›åŠŸèƒ½ä¸ºæ„å»ºå¤æ‚çš„å¤§æ¨¡å‹åº”ç”¨æä¾›äº†å¼ºå¤§æ”¯æŒï¼š\n",
    "æ•°æ®è¿æ¥å°è£…ï¼šæä¾›äº†å„ç§æ–‡æ¡£åŠ è½½å™¨ã€æ–‡æœ¬åˆ†å‰²å™¨å’Œå‘é‡å­˜å‚¨æ¥å£ï¼Œæ–¹ä¾¿å¤„ç†å„ç§æ ¼å¼çš„æ–‡æ¡£å’Œå®ç°è¯­ä¹‰æœç´¢ã€‚\n",
    "LCELï¼šæä¾›äº†ä¸€ç§å£°æ˜å¼è¯­è¨€ï¼Œç®€åŒ–äº†Chainçš„æ„å»ºè¿‡ç¨‹ï¼Œæ”¯æŒå„ç§å¤æ‚çš„åº”ç”¨æ¨¡å¼ï¼Œå¦‚RAGã€å·¥å‚æ¨¡å¼ç­‰ã€‚\n",
    "\n",
    "åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»LangChainçš„å·¥ä½œæµæ¡†æ¶LangGraphï¼Œå®ƒä¸ºæ„å»ºæ›´å¤æ‚çš„å¤§æ¨¡å‹åº”ç”¨æä¾›äº†æ›´é«˜çº§çš„æ”¯æŒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda12_0_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
