{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChainä»å…¥é—¨åˆ°å®è·µï¼šæ„å»ºæ™ºèƒ½åº”ç”¨çš„åŸºç¡€æ¡†æ¶\n",
    "\n",
    "LangChainæ˜¯å½“å‰å¤§æ¨¡å‹åº”ç”¨å¼€å‘ä¸­æœ€æµè¡Œçš„æ¡†æ¶ä¹‹ä¸€ï¼Œå®ƒä¸ºå¼€å‘è€…æä¾›äº†ä¸€å¥—å®Œæ•´çš„å·¥å…·é“¾ï¼Œå¸®åŠ©æˆ‘ä»¬æ›´ä¾¿æ·åœ°æ„å»ºåŸºäºå¤§æ¨¡å‹çš„åº”ç”¨ã€‚æœ¬æ–‡å°†å¸¦é¢†è¯»è€…å…¥é—¨LangChainï¼ŒæŒæ¡å…¶æ ¸å¿ƒæ¦‚å¿µå’ŒåŸºç¡€ç»„ä»¶ã€‚\n",
    "\n",
    "## 1. LangChainç®€ä»‹\n",
    "\n",
    "LangChainæ˜¯ä¸€ä¸ªé¢å‘å¤§æ¨¡å‹çš„å¼€å‘æ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–å¤§æ¨¡å‹åº”ç”¨çš„å¼€å‘è¿‡ç¨‹ã€‚å®ƒçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºï¼š\n",
    "ç»Ÿä¸€äº†ä¸åŒæ¨¡å‹çš„è°ƒç”¨æ¥å£\n",
    "æä¾›äº†ä¸°å¯Œçš„ç»„ä»¶å’Œå·¥å…·é“¾\n",
    "æ”¯æŒé“¾å¼å¤„ç†æµç¨‹\n",
    "ä¾¿äºé›†æˆå¤–éƒ¨çŸ¥è¯†å’Œå·¥å…·\n",
    "\n",
    "LangChainçš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼š\n",
    "\n",
    "æ¨¡å‹I/Oå°è£…ï¼šChat Modelsã€PromptTemplateã€OutputParserç­‰\n",
    "æ•°æ®è¿æ¥å°è£…ï¼šDocument Loadersã€Document Transformersã€Text Embedding Modelsã€Vectorstores & Retrieversç­‰\n",
    "æ¶æ„å°è£…ï¼šChain/LCELã€Agentã€Toolsã€LangGraphã€LangSmithç­‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ç¯å¢ƒå‡†å¤‡\n",
    "\n",
    "åœ¨å¼€å§‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆå®‰è£…å¿…è¦çš„ä¾èµ–ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (0.3.12)\n",
      "Requirement already satisfied: python-dotenv in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-openai) (0.3.51)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-openai) (1.70.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.3.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (2.10.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: sniffio in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.2.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.4)\n",
      "Requirement already satisfied: certifi in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain-openai) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
      "Requirement already satisfied: colorama in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.68.2->langchain-openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£…LangChainå’ŒOpenAIåŒ…\n",
    "%pip install langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥åˆ›å»º.envæ–‡ä»¶ï¼Œé…ç½®OpenAI APIå¯†é’¥ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPENAI_API_KEY=your_api_key_here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ¨¡å‹I/Oå°è£…\n",
    "\n",
    "### 3.1 æ¨¡å‹æ¥å£å°è£…\n",
    "\n",
    "LangChainå¯¹å„ç§å¤§æ¨¡å‹æä¾›äº†ç»Ÿä¸€çš„æ¥å£ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿè½»æ¾åˆ‡æ¢ä¸åŒçš„æ¨¡å‹è€Œä¸ç”¨å¤§å¹…ä¿®æ”¹ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨å›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯å’ŒååŠ©è§£å†³é—®é¢˜ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œéšæ—¶å¯ä»¥é—®æˆ‘ï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡æ˜¯å¦å­˜åœ¨\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®OPENAI_API_KEY\")\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "response = model.invoke(\"ä½ æ˜¯è°\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 å¤šè½®å¯¹è¯å°è£…\n",
    "\n",
    "LangChainæä¾›äº†æ ‡å‡†åŒ–çš„æ¶ˆæ¯ç±»å‹ï¼Œç”¨äºå¤„ç†å¤šè½®å¯¹è¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ æ˜¯å­¦å‘˜ kleinSperoã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–è€…éœ€è¦å¸®åŠ©ï¼Œéšæ—¶å¯ä»¥å‘Šè¯‰æˆ‘ï¼\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„assistant role\n",
    "    HumanMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„user role\n",
    "    SystemMessage  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„system role\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯äº‘å»·ç§‘æŠ€çš„è¯¾ç¨‹åŠ©ç†ã€‚\"),\n",
    "    HumanMessage(content=\"æˆ‘æ˜¯å­¦å‘˜ï¼Œæˆ‘å«kleinSperoã€‚\"),\n",
    "    AIMessage(content=\"æ¬¢è¿ï¼\"),\n",
    "    HumanMessage(content=\"æˆ‘æ˜¯è°\")\n",
    "]\n",
    "\n",
    "ret = model.invoke(messages)\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 æ›´æ¢æ¨¡å‹æä¾›å•†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChainçš„ä¸€å¤§ä¼˜åŠ¿æ˜¯å¯ä»¥è½»æ¾åˆ‡æ¢ä¸åŒçš„æ¨¡å‹æä¾›å•†ã€‚\n",
    "\n",
    "ä»¥DeepSeekæ¨¡å‹ä¸ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-deepseek in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (0.1.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.47 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-deepseek) (0.3.51)\n",
      "Requirement already satisfied: langchain-openai<1.0.0,>=0.3.9 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-deepseek) (0.3.12)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (0.3.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (2.10.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (1.70.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (0.8.0)\n",
      "Requirement already satisfied: sniffio in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (2.27.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (2024.5.15)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (1.2.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (3.4)\n",
      "Requirement already satisfied: certifi in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (2.3.0)\n",
      "Requirement already satisfied: colorama in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„æ™ºèƒ½åŠ©æ‰‹ï¼âœ¨ æˆ‘çš„ä½¿å‘½æ˜¯å¸®åŠ©ä½ è§£ç­”å„ç§é—®é¢˜ï¼Œæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œï¼Œè¿˜æ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å°å›°æƒ‘ã€‚æˆ‘å¯ä»¥é™ªä½ èŠå¤©ã€æä¾›å»ºè®®ã€æŸ¥æ‰¾èµ„æ–™ï¼Œç”šè‡³å¸®ä½ åˆ†ææ–‡ä»¶å†…å®¹ã€‚  \n",
      "\n",
      "æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£…DeepSeekæ”¯æŒ\n",
    "# %pip install langchain-deepseek\n",
    "\n",
    "# ä½¿ç”¨DeepSeekæ¨¡å‹\n",
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡æ˜¯å¦å­˜åœ¨\n",
    "if not os.getenv(\"DEEPSEEK_API_KEY\"):\n",
    "    raise ValueError(\"è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®DEEPSEEK_API_KEY\")\n",
    "\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "response = model.invoke(\"ä½ æ˜¯è°\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥Qwenä¸ºä¾‹:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€é‚®ä»¶ã€å‰§æœ¬ç­‰ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ã€ç¼–ç¨‹ï¼Œç”šè‡³è¡¨è¾¾è§‚ç‚¹å’Œç©æ¸¸æˆã€‚æˆ‘åœ¨å¤šå›½è¯­è¨€ä¸Šéƒ½æœ‰å¾ˆå¥½çš„æŒæ¡ï¼Œèƒ½ä¸ºä½ æä¾›å¤šæ ·åŒ–çš„å¸®åŠ©ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åˆ°ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨åƒé—®æ¨¡å‹\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.llms import Tongyi\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡æ˜¯å¦å­˜åœ¨\n",
    "if not os.getenv(\"DASHSCOPE_API_KEY\"):\n",
    "    raise ValueError(\"è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®DASHSCOPE_API_KEY\")\n",
    "\n",
    "# åŠ è½½ Tongyi æ¨¡å‹\n",
    "model = Tongyi(model_name=\"qwen-turbo\", dashscope_api_key=os.getenv(\"DASHSCOPE_API_KEY\"))  # ä½¿ç”¨é€šä¹‰åƒé—®qwen-turboæ¨¡å‹\n",
    "\n",
    "response = model.invoke(\"ä½ æ˜¯è°\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 æµå¼è¾“å‡º\n",
    "\n",
    "å¯¹äºé•¿å›ç­”ï¼Œæµå¼è¾“å‡ºå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openaiã€deepseek\n",
    "for token in model.stream(\"ä½ æ˜¯è°\"):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®éªŒå®¤è‡ªä¸»ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€é‚®ä»¶ã€å‰§æœ¬ç­‰ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ã€ç¼–ç¨‹ç­‰ä»»åŠ¡ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼"
     ]
    }
   ],
   "source": [
    "# é€šä¹‰åƒé—®\n",
    "for token in model.stream(\"ä½ æ˜¯è°\"):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Promptæ¨¡æ¿\n",
    "\n",
    "### 4.1 åŸºç¡€Promptæ¨¡æ¿\n",
    "\n",
    "Promptæ¨¡æ¿å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç»Ÿä¸€ç®¡ç†æç¤ºè¯ï¼Œå¹¶æ”¯æŒå˜é‡æ›¿æ¢ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['subject'] input_types={} partial_variables={} template='ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯'\n",
      "===Prompt===\n",
      "ç»™æˆ‘è®²ä¸ªå…³äºå°æ˜çš„ç¬‘è¯\n",
      "å°æ˜å’Œä»–çš„æœ‹å‹ä»¬ä¸€èµ·å»éƒŠæ¸¸ï¼Œå¤§å®¶éƒ½å¸¦äº†å¥½åƒçš„é£Ÿç‰©ã€‚å°æ˜å¸¦äº†ä¸€ç­è‹¹æœã€‚\n",
      "\n",
      "å¤§å®¶ååœ¨ä¸€èµ·ï¼Œæœ‹å‹ä»¬å¼€å§‹åˆ†äº«è‡ªå·±çš„ç¾é£Ÿã€‚å°æ˜ä¸€è¾¹åƒè‹¹æœï¼Œä¸€è¾¹çœ‹ç€æœ‹å‹ä»¬çš„ç¾å‘³ä½³è‚´ï¼Œå¿ƒé‡Œæœ‰ç‚¹ç¾¡æ…•ã€‚\n",
      "\n",
      "ä»–å°±çªç„¶è¯´ï¼šâ€œæˆ‘è§‰å¾—æˆ‘çš„è‹¹æœä¹Ÿå¾ˆå¥½åƒï¼â€\n",
      "\n",
      "å¤§å®¶å¥½å¥‡åœ°é—®ï¼šâ€œé‚£ä½ ä¸ºä»€ä¹ˆä¸åˆ†äº«å‘¢ï¼Ÿâ€\n",
      "\n",
      "å°æ˜ç¬‘ç€å›ç­”ï¼šâ€œå› ä¸ºâ€¦æˆ‘æ€•æˆ‘çš„è‹¹æœä¼šè¢«å·ï¼è¿™å¯æ˜¯æˆ‘çš„ç§˜å¯†æ­¦å™¨ï¼â€\n",
      "\n",
      "æœ‹å‹ä»¬ç–‘æƒ‘åœ°é—®ï¼šâ€œä»€ä¹ˆç§˜å¯†æ­¦å™¨ï¼Ÿâ€\n",
      "\n",
      "å°æ˜å¾—æ„åœ°è¯´ï¼šâ€œå› ä¸ºè¿™æ˜¯æˆ‘å¦ˆç‰¹æ„ä¸ºæˆ‘é€‰çš„ï¼Œå› ä¸ºå¥¹è¯´è‹¹æœå¯ä»¥è®©äººèªæ˜ï¼æ‰€ä»¥æˆ‘åªæƒ³è‡ªå·±å·å·äº«ç”¨ï¼â€\n",
      "\n",
      "æœ‹å‹ä»¬å“ˆå“ˆå¤§ç¬‘ï¼Œè¯´ï¼šâ€œé‚£æˆ‘ä»¬å°±ç¥ä½ çš„ç§˜å¯†æ­¦å™¨èƒ½è®©ä½ å˜å¾—æ›´èªæ˜å§ï¼åˆ«å¿˜äº†ä¹Ÿè¦å’Œæˆ‘ä»¬åˆ†äº«å“¦ï¼â€\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(subject='å°æ˜'))\n",
    "\n",
    "# ä½¿ç”¨æ¨¡æ¿è°ƒç”¨LLM\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "ret = llm.invoke(template.format(subject='å°æ˜'))\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ChatPromptTemplate\n",
    "\n",
    "ChatPromptTemplateç”¨äºæ„å»ºå¯¹è¯å¼çš„æç¤ºæ¨¡æ¿ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='ä½ æ˜¯AIå¤§æ¨¡å‹åº”ç”¨å¼€å‘çš„å®¢æœåŠ©æ‰‹ã€‚ä½ çš„åå­—å«å°ç¨‹', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ æ˜¯è°', additional_kwargs={}, response_metadata={})]\n",
      "ä½ å¯ä»¥å«æˆ‘å°ç¨‹ï¼Œæˆ‘æ˜¯AIå¤§æ¨¡å‹åº”ç”¨å¼€å‘çš„å®¢æœåŠ©æ‰‹ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"ä½ æ˜¯{product}çš„å®¢æœåŠ©æ‰‹ã€‚ä½ çš„åå­—å«{name}\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    product=\"AIå¤§æ¨¡å‹åº”ç”¨å¼€å‘\",\n",
    "    name=\"å°ç¨‹\",\n",
    "    query=\"ä½ æ˜¯è°\"\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "ret = llm.invoke(prompt)\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 MessagesPlaceholder\n",
    "\n",
    "MessagesPlaceholderç”¨äºåœ¨æ¨¡æ¿ä¸­é¢„ç•™ä½ç½®ï¼Œä»¥ä¾¿åç»­å¡«å……å†å²æ¶ˆæ¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Who is Elon Musk?', additional_kwargs={}, response_metadata={}), AIMessage(content='Elon Musk is a billionaire entrepreneur, inventor, and industrial designer', additional_kwargs={}, response_metadata={}), HumanMessage(content='Translate your answer to ä¸­æ–‡.', additional_kwargs={}, response_metadata={})]\n",
      "åŸƒéš†Â·é©¬æ–¯å…‹ï¼ˆElon Muskï¼‰æ˜¯ä¸€ä½äº¿ä¸‡å¯Œç¿ä¼ä¸šå®¶ã€å‘æ˜å®¶å’Œå·¥ä¸šè®¾è®¡å¸ˆã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "human_prompt = \"Translate your answer to {language}.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [MessagesPlaceholder(\"history\"), human_message_template]\n",
    ")\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "# åˆ›å»ºç¤ºä¾‹å¯¹è¯\n",
    "human_message = HumanMessage(content=\"Who is Elon Musk?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"Elon Musk is a billionaire entrepreneur, inventor, and industrial designer\"\n",
    ")\n",
    "# ä½¿ç”¨æ¨¡æ¿æ ¼å¼åŒ–æ¶ˆæ¯\n",
    "messages = chat_prompt.format_prompt(\n",
    "    history=[human_message, ai_message], language=\"ä¸­æ–‡\"\n",
    ")\n",
    "# æ‰“å°æ ¼å¼åŒ–åçš„æ¶ˆæ¯\n",
    "print(messages.to_messages())\n",
    "\n",
    "result = llm.invoke(messages) \n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 ä»æ–‡ä»¶åŠ è½½Promptæ¨¡æ¿\n",
    "\n",
    "å¯¹äºå¤æ‚çš„Promptï¼Œå¯ä»¥ä»æ–‡ä»¶åŠ è½½ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['topic'] input_types={} partial_variables={} template='ä¸¾ä¸€ä¸ªå…³äº{topic}çš„ä¾‹å­'\n",
      "===Prompt===\n",
      "ä¸¾ä¸€ä¸ªå…³äºé»‘è‰²å¹½é»˜çš„ä¾‹å­\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# å‡è®¾æ–‡ä»¶å†…å®¹ä¸º: ä¸¾ä¸€ä¸ªå…³äº{topic}çš„ä¾‹å­\n",
    "template = PromptTemplate.from_file(\"./data/prompt/example_prompt_template.txt\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(topic='é»‘è‰²å¹½é»˜'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ç»“æ„åŒ–è¾“å‡º\n",
    "\n",
    "### 5.1 ä½¿ç”¨Pydanticå¯¹è±¡\n",
    "\n",
    "LangChainå¯ä»¥ç›´æ¥è¾“å‡ºPydanticå¯¹è±¡ï¼Œä½¿APIäº¤äº’æ›´åŠ ä¾¿æ·ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year=2023 month=4 day=6 era='AD'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# å®šä¹‰è¾“å‡ºå¯¹è±¡ç»“æ„\n",
    "class Date(BaseModel):\n",
    "    year: int = Field(description=\"Year\")\n",
    "    month: int = Field(description=\"Month\")\n",
    "    day: int = Field(description=\"Day\")\n",
    "    era: str = Field(description=\"BC or AD\")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "# å®šä¹‰ç»“æ„åŒ–è¾“å‡ºçš„æ¨¡å‹\n",
    "structured_llm = llm.with_structured_output(Date)\n",
    "\n",
    "template = \"\"\"æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\n",
    "ç”¨æˆ·è¾“å…¥:\n",
    "{query}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "query = \"2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´...\"\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "\n",
    "result = structured_llm.invoke(input_prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 è¾“å‡ºJSONæ ¼å¼\n",
    "\n",
    "ä¹Ÿå¯ä»¥ä½¿ç”¨JSON Schemaå®šä¹‰è¾“å‡ºæ ¼å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'year': 2023, 'month': 4, 'day': 6, 'era': 'AD'}\n"
     ]
    }
   ],
   "source": [
    "json_schema = {\n",
    "    \"title\": \"Date\",\n",
    "    \"description\": \"Formated date expression\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"year\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"year, YYYY\",\n",
    "        },\n",
    "        \"month\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"month, MM\",\n",
    "        },\n",
    "        \"day\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"day, DD\",\n",
    "        },\n",
    "        \"era\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"BC or AD\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "structured_llm = llm.with_structured_output(json_schema)\n",
    "\n",
    "result = structured_llm.invoke(input_prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 ä½¿ç”¨OutputParser\n",
    "\n",
    "OutputParserå¯ä»¥è§£æå¤§æ¨¡å‹çš„è¾“å‡ºä¸ºæŒ‡å®šæ ¼å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹è¾“å‡º:\n",
      "```json\n",
      "{\"year\": 2023, \"month\": 4, \"day\": 6, \"era\": \"AD\"}\n",
      "```\n",
      "\n",
      "è§£æå:\n",
      "{'year': 2023, 'month': 4, 'day': 6, 'era': 'AD'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Date)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\\nç”¨æˆ·è¾“å…¥:{query}\\n{format_instructions}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "output = llm.invoke(input_prompt)\n",
    "print(\"åŸå§‹è¾“å‡º:\\n\"+output.content)\n",
    "\n",
    "print(\"\\nè§£æå:\")\n",
    "print(parser.invoke(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¹Ÿå¯ä»¥ä½¿ç”¨PydanticOutputParserï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹è¾“å‡º:\n",
      "```json\n",
      "{\"year\": 2023, \"month\": 4, \"day\": 6, \"era\": \"AD\"}\n",
      "```\n",
      "\n",
      "è§£æå:\n",
      "year=2023 month=4 day=6 era='AD'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Date)\n",
    "\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "output = llm.invoke(input_prompt)\n",
    "print(\"åŸå§‹è¾“å‡º:\\n\"+output.content)\n",
    "\n",
    "print(\"\\nè§£æå:\")\n",
    "print(parser.invoke(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 è¾“å‡ºæ ¼å¼çº é”™\n",
    "\n",
    "OutputFixingParserå¯ä»¥ä¿®å¤æ ¼å¼ä¸æ­£ç¡®çš„è¾“å‡ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PydanticOutputParser:\n",
      "Invalid json output: ```json\n",
      "{\"year\": 2023, \"month\": å››, \"day\": 6, \"era\": \"AD\"}\n",
      "```\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "OutputFixingParser:\n",
      "year=2023 month=4 day=6 era='AD'\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# åˆ›å»ºä¿®å¤è§£æå™¨\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())\n",
    "# åˆ›å»ºé”™è¯¯æ•°æ®å¹¶æµ‹è¯•\n",
    "bad_output = output.content.replace(\"4\",\"å››\")\n",
    "# ä½¿ç”¨åŸå§‹è§£æå™¨å°è¯•è§£æ\n",
    "print(\"PydanticOutputParser:\")\n",
    "try:\n",
    "    parser.invoke(bad_output)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "# ä½¿ç”¨æ–°çš„OutputFixingParserè§£æåŒæ ·çš„é”™è¯¯æ•°æ®\n",
    "print(\"OutputFixingParser:\")\n",
    "print(new_parser.invoke(bad_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Function Calling\n",
    "\n",
    "Function Callingæ˜¯å¤§æ¨¡å‹çš„é‡è¦èƒ½åŠ›ï¼ŒLangChainæä¾›äº†ä¾¿æ·çš„æ¥å£ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"name\": \"multiply\",\n",
      "        \"args\": {\n",
      "            \"a\": 3,\n",
      "            \"b\": 4\n",
      "        },\n",
      "        \"id\": \"call_ineEpBk9521v5Bh7TXJDTu8c\",\n",
      "        \"type\": \"tool_call\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# ä½¿ç”¨@toolè£…é¥°å™¨å°†æ™®é€šå‡½æ•°è½¬æ¢ä¸ºLangChainå·¥å…·\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.  \n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "# ç»‘å®šå·¥å…·åˆ°æ¨¡å‹\n",
    "import json\n",
    "\n",
    "llm_with_tools = llm.bind_tools([add, multiply])\n",
    "\n",
    "query = \"3çš„4å€æ˜¯å¤šå°‘?\"\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "output = llm_with_tools.invoke(messages)\n",
    "print(json.dumps(output.tool_calls, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¤„ç†å·¥å…·è°ƒç”¨ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3çš„4å€æ˜¯12ã€‚\n"
     ]
    }
   ],
   "source": [
    "# æ‰§è¡Œå·¥å…·è°ƒç”¨\n",
    "messages.append(output)\n",
    "\n",
    "available_tools = {\"add\": add, \"multiply\": multiply}\n",
    "\n",
    "for tool_call in output.tool_calls:\n",
    "    selected_tool = available_tools[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "# å†æ¬¡è°ƒç”¨æ¨¡å‹å¤„ç†å·¥å…·ç»“æœ\n",
    "result = llm_with_tools.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ€»ç»“\n",
    "\n",
    "æœ¬æ–‡ä»‹ç»äº†LangChainçš„åŸºç¡€æ¦‚å¿µå’Œæ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬æ¨¡å‹å°è£…ã€Promptæ¨¡æ¿ã€ç»“æ„åŒ–è¾“å‡ºå’ŒFunction Callingç­‰ã€‚è¿™äº›ç»„ä»¶ä¸ºæˆ‘ä»¬æ„å»ºå¤§æ¨¡å‹åº”ç”¨æä¾›äº†å¼ºå¤§çš„åŸºç¡€ã€‚\n",
    "åœ¨åç»­æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ç»§ç»­æ¢ç´¢LangChainçš„é«˜çº§ç‰¹æ€§ï¼ŒåŒ…æ‹¬æ•°æ®è¿æ¥ã€å‘é‡æ£€ç´¢ã€Chainæ„å»ºå’ŒAgentå¼€å‘ç­‰å†…å®¹ï¼Œæ•¬è¯·æœŸå¾…ï¼\n",
    "\n",
    "æ³¨æ„ï¼šæœ¬æ–‡ä»£ç ä½¿ç”¨äº†æœ€æ–°çš„LangChain APIï¼Œå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯è¾ƒæ—©ç‰ˆæœ¬ï¼Œå¯èƒ½éœ€è¦è¿›è¡Œç›¸åº”è°ƒæ•´ã€‚å»ºè®®ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„LangChainä»¥è·å¾—æœ€ä½³ä½“éªŒ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda12_0_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
