{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain从入门到实践：数据连接与流程编排\n",
    "\n",
    "在上一篇文章中，我们介绍了LangChain的基础组件，包括模型封装、Prompt模板、结构化输出和Function Calling。本篇文章将继续深入，为大家介绍LangChain的数据连接能力和流程编排功能，帮助你构建更复杂的大模型应用。\n",
    "\n",
    "注意事项：\n",
    "运行本文代码前，请确保已安装必要的依赖包：pip install langchain-openai langchain-text-splitters langchain-community faiss-cpu pymupdf langchain-dashscope\n",
    "请在.env文件中配置相应的API密钥\n",
    "示例中的文件路径需要替换为你自己的文件路径\n",
    "\n",
    "## 1. 数据连接封装\n",
    "\n",
    "大模型应用通常需要连接外部数据源，LangChain提供了丰富的数据加载和处理组件，让我们可以轻松处理各种格式的文档。\n",
    "\n",
    "### 1.1 文档加载器：Document Loaders\n",
    "\n",
    "文档加载器负责从不同来源加载文档，以下是使用PyMuPDF加载PDF文档的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗\n",
      "Louis Martin†\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov\n",
      "Thomas Scialom∗\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "source models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "†Second author\n",
      "Contributions for all the authors can be found in Section A.1.\n",
      "arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n"
     ]
    }
   ],
   "source": [
    "# 安装必要的依赖\n",
    "# %pip install pymupdf langchain-openai langchain-text-splitters faiss-cpu\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# 加载PDF文档\n",
    "loader = PyMuPDFLoader(\"./data/rag_data/llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# 打印第一页内容\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain支持多种文档格式，包括PDF、Word、CSV、HTML等，你可以根据需要选择合适的加载器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 文本分割器\n",
    "\n",
    "文本分割器用于将长文档切分成适合大模型处理的小块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗\n",
      "Louis Martin†\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "-------\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "-------\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "-------\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "-------\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "-------\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov\n",
      "Thomas Scialom∗\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "-------\n",
      "Sergey Edunov\n",
      "Thomas Scialom∗\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "-------\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "-------\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "source models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "-------\n",
      "source models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "-------\n",
      "contribute to the responsible development of LLMs.\n",
      "∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "†Second author\n",
      "Contributions for all the authors can be found in Section A.1.\n",
      "arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 创建文本分割器\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,         # 每个文本块的目标大小（字符数）\n",
    "    chunk_overlap=100,      # 相邻文本块之间的重叠部分大小\n",
    "    length_function=len,    # 用于计算文本长度的函数\n",
    "    add_start_index=True,   # 是否添加起始索引信息\n",
    ")\n",
    "\n",
    "# 分割文本\n",
    "paragraphs = text_splitter.create_documents([pages[0].page_content])\n",
    "for para in paragraphs:\n",
    "    print(para.page_content)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 向量数据库与向量检索\n",
    "\n",
    "LangChain提供了与多种向量数据库的集成，方便实现语义搜索："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but are not releasing.§\n",
      "2. Llama 2-Chat, a ﬁne-tuned version of Llama 2 that is optimized for dialogue use cases. We release\n",
      "variants of this model with 7B, 13B, and 70B parameters as well.\n",
      "We believe that the open release of LLMs, when done safely, will be a net beneﬁt to society. Like all LLMs,\n",
      "----\n",
      "Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "Llama 2-Chat models generally perform better than existing open-source models. They also appear to\n",
      "----\n",
      "Sergey Edunov\n",
      "Thomas Scialom∗\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# 加载文档\n",
    "loader = PyMuPDFLoader(\"./data/rag_data/llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# 文档切分\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:4]]\n",
    ")\n",
    "\n",
    "# 创建向量数据库\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# 检索相关文档\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})  # 检索top-3相关文档\n",
    "query = \"llama2有多少参数?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# 打印检索结果\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chain和LangChain Expression Language (LCEL)\n",
    "\n",
    "Chain是LangChain的核心概念之一，它允许我们将多个组件连接起来，形成一个完整的处理流程。LangChain Expression Language (LCEL)是LangChain提供的一种声明式语言，用于简化Chain的构建过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 使用LCEL构建简单Pipeline\n",
    "\n",
    "LCEL允许我们使用管道符|将不同组件连接起来，形成处理流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": null,\n",
      "    \"price_lower\": null,\n",
      "    \"price_upper\": 100,\n",
      "    \"data_lower\": null,\n",
      "    \"data_upper\": null,\n",
      "    \"sort_by\": \"data\",\n",
      "    \"ordering\": \"descend\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "import json\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# 定义输出结构\n",
    "class SortEnum(str, Enum):\n",
    "    data = 'data'   # 按流量排序\n",
    "    price = 'price' # 按价格排序\n",
    "\n",
    "class OrderingEnum(str, Enum):\n",
    "    ascend = 'ascend'   # 升序\n",
    "    descend = 'descend' # 降序\n",
    "\n",
    "class Semantics(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"流量包名称\", default=None)\n",
    "    price_lower: Optional[int] = Field(description=\"价格下限\", default=None)\n",
    "    price_upper: Optional[int] = Field(description=\"价格上限\", default=None)\n",
    "    data_lower: Optional[int] = Field(description=\"流量下限\", default=None)\n",
    "    data_upper: Optional[int] = Field(description=\"流量上限\", default=None)\n",
    "    sort_by: Optional[SortEnum] = Field(description=\"按价格或流量排序\", default=None)\n",
    "    ordering: Optional[OrderingEnum] = Field(description=\"升序或降序排列\", default=None)\n",
    "\n",
    "# Prompt模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个语义解析器。你的任务是将用户的输入解析成JSON表示。不要回答用户的问题。\"),\n",
    "    (\"human\", \"{text}\"),\n",
    "])\n",
    "\n",
    "# 模型\n",
    "llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "structured_llm = llm.with_structured_output(Semantics)\n",
    "\n",
    "# LCEL表达式\n",
    "runnable = (\n",
    "    {\"text\": RunnablePassthrough()} | prompt | structured_llm\n",
    ")\n",
    "\n",
    "# 运行\n",
    "ret = runnable.invoke(\"不超过100元的流量大的套餐有哪些\")\n",
    "print(json.dumps(ret.model_dump(), indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 使用LCEL实现RAG\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) 是大模型应用的一种重要模式，LCEL可以帮助我们轻松实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的内容，文档中列出了以下部分的重要信息：\n",
      "\n",
      "1. **数据预训练（Pretraining）**：\n",
      "   - 前馈数据（Pretraining Data）\n",
      "   - 训练细节（Training Details）\n",
      "\n",
      "2. **附录部分**：\n",
      "   - 数据标注（Data Annotation）\n",
      "   - 数据集污染（Dataset Contamination）\n",
      "   - 模型卡（Model Card）\n",
      "\n",
      "这些部分涉及到模型的训练过程及相关数据的信息。\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# 加载文档\n",
    "loader = PyMuPDFLoader(\"./data/rag_data/llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# 文档切分\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:4]]\n",
    ")\n",
    "\n",
    "# 创建向量数据库\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Prompt模板\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 构建RAG Chain\n",
    "rag_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 运行RAG\n",
    "answer = rag_chain.invoke(\"文档中有什么重要信息\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 使用LCEL实现工厂模式\n",
    "\n",
    "LCEL还支持工厂模式，可以根据需要动态选择使用的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-dashscope\n",
      "  Downloading langchain_dashscope-0.1.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: dashscope<2.0.0,>=1.14.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from langchain-dashscope) (1.22.2)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from langchain-dashscope)\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from dashscope<2.0.0,>=1.14.1->langchain-dashscope) (3.11.12)\n",
      "Requirement already satisfied: requests in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from dashscope<2.0.0,>=1.14.1->langchain-dashscope) (2.32.3)\n",
      "Requirement already satisfied: websocket-client in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from dashscope<2.0.0,>=1.14.1->langchain-dashscope) (1.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from tiktoken<0.8.0,>=0.7.0->langchain-dashscope) (2024.5.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp->dashscope<2.0.0,>=1.14.1->langchain-dashscope) (4.12.2)\n",
      "Downloading langchain_dashscope-0.1.8-py3-none-any.whl (9.0 kB)\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-win_amd64.whl (798 kB)\n",
      "   ---------------------------------------- 0.0/798.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/798.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/798.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 20.5/798.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 20.5/798.9 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/798.9 kB 262.6 kB/s eta 0:00:03\n",
      "   - ------------------------------------- 30.7/798.9 kB 262.6 kB/s eta 0:00:03\n",
      "   - ------------------------------------- 41.0/798.9 kB 178.6 kB/s eta 0:00:05\n",
      "   - ------------------------------------- 41.0/798.9 kB 178.6 kB/s eta 0:00:05\n",
      "   -- ------------------------------------ 61.4/798.9 kB 218.8 kB/s eta 0:00:04\n",
      "   -- ------------------------------------ 61.4/798.9 kB 218.8 kB/s eta 0:00:04\n",
      "   --- ----------------------------------- 81.9/798.9 kB 229.0 kB/s eta 0:00:04\n",
      "   ----- -------------------------------- 112.6/798.9 kB 297.7 kB/s eta 0:00:03\n",
      "   ------ ------------------------------- 143.4/798.9 kB 327.9 kB/s eta 0:00:02\n",
      "   ------- ------------------------------ 163.8/798.9 kB 350.7 kB/s eta 0:00:02\n",
      "   --------- ---------------------------- 194.6/798.9 kB 368.6 kB/s eta 0:00:02\n",
      "   ----------- -------------------------- 245.8/798.9 kB 430.6 kB/s eta 0:00:02\n",
      "   ------------ ------------------------- 256.0/798.9 kB 436.5 kB/s eta 0:00:02\n",
      "   ------------- ------------------------ 276.5/798.9 kB 448.2 kB/s eta 0:00:02\n",
      "   ---------------- --------------------- 337.9/798.9 kB 499.5 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 389.1/798.9 kB 550.7 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 491.5/798.9 kB 641.7 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 532.5/798.9 kB 668.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 583.7/798.9 kB 692.6 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 686.1/798.9 kB 786.8 kB/s eta 0:00:01\n",
      "   -------------------------------------  778.2/798.9 kB 847.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 798.9/798.9 kB 840.8 kB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken, langchain-dashscope\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.9.0\n",
      "    Uninstalling tiktoken-0.9.0:\n",
      "      Successfully uninstalled tiktoken-0.9.0\n",
      "Successfully installed langchain-dashscope-0.1.8 tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\Anacanda3\\envs\\pytorch_cuda12_0_py310\\Lib\\site-packages\\~iktoken'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-dashscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qianfan\n",
      "  Downloading qianfan-0.4.12.3-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: aiohttp>=3.7.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (3.11.12)\n",
      "Collecting aiolimiter>=1.1.0 (from qianfan)\n",
      "  Downloading aiolimiter-1.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: bce-python-sdk>=0.8.79 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (0.9.29)\n",
      "Requirement already satisfied: cachetools>=5.0.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (5.5.2)\n",
      "Collecting diskcache>=5.6.3 (from qianfan)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting multiprocess>=0.70.12 (from qianfan)\n",
      "  Downloading multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.38 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (3.0.47)\n",
      "Requirement already satisfied: pydantic>=1.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (2.10.2)\n",
      "Requirement already satisfied: python-dotenv>=1.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (1.0.1)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.24 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (2.32.3)\n",
      "Requirement already satisfied: rich>=13.0.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (13.9.4)\n",
      "Collecting tenacity<9.0.0,>=8.2.3 (from qianfan)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from qianfan) (0.15.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from aiohttp>=3.7.0->qianfan) (1.18.3)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from bce-python-sdk>=0.8.79->qianfan) (3.22.0)\n",
      "Requirement already satisfied: future>=0.6.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from bce-python-sdk>=0.8.79->qianfan) (1.0.0)\n",
      "Requirement already satisfied: six>=1.4.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from bce-python-sdk>=0.8.79->qianfan) (1.16.0)\n",
      "Collecting dill>=0.4.0 (from multiprocess>=0.70.12->qianfan)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: wcwidth in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from prompt-toolkit>=3.0.38->qianfan) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from pydantic>=1.0->qianfan) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from pydantic>=1.0->qianfan) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from pydantic>=1.0->qianfan) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests>=2.24->qianfan) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests>=2.24->qianfan) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests>=2.24->qianfan) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from requests>=2.24->qianfan) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from rich>=13.0.0->qianfan) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from rich>=13.0.0->qianfan) (2.18.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from typer>=0.9.0->qianfan) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from typer>=0.9.0->qianfan) (1.5.4)\n",
      "Requirement already satisfied: colorama in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from click>=8.0.0->typer>=0.9.0->qianfan) (0.4.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\anacanda3\\envs\\pytorch_cuda12_0_py310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.0.0->qianfan) (0.1.2)\n",
      "Downloading qianfan-0.4.12.3-py3-none-any.whl (470 kB)\n",
      "   ---------------------------------------- 0.0/470.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/470.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/470.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/470.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/470.3 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 41.0/470.3 kB 1.9 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 41.0/470.3 kB 1.9 MB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 51.2/470.3 kB 435.7 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 71.7/470.3 kB 491.5 kB/s eta 0:00:01\n",
      "   ------ -------------------------------- 81.9/470.3 kB 381.3 kB/s eta 0:00:02\n",
      "   --------- ---------------------------- 122.9/470.3 kB 481.4 kB/s eta 0:00:01\n",
      "   ---------- --------------------------- 133.1/470.3 kB 462.0 kB/s eta 0:00:01\n",
      "   ------------ ------------------------- 153.6/470.3 kB 482.7 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 184.3/470.3 kB 506.0 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 215.0/470.3 kB 546.1 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 266.2/470.3 kB 584.5 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 327.7/470.3 kB 676.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 399.4/470.3 kB 754.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 470.3/470.3 kB 817.1 kB/s eta 0:00:00\n",
      "Downloading aiolimiter-1.2.1-py3-none-any.whl (6.7 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.5/45.5 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.18-py310-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.9 kB ? eta -:--:--\n",
      "   ---------------------------------------  133.1/134.9 kB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 134.9/134.9 kB 2.7 MB/s eta 0:00:00\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "   ---------------------------------------- 0.0/119.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 119.7/119.7 kB 7.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tenacity, diskcache, dill, aiolimiter, multiprocess, qianfan\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "Successfully installed aiolimiter-1.2.1 dill-0.4.0 diskcache-5.6.3 multiprocess-0.70.18 qianfan-0.4.12.3 tenacity-8.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install qianfan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是 **DeepSeek Chat**，由深度求索公司（DeepSeek）研发的智能AI助手。我可以帮助你解答各种问题，包括学习、工作、编程、写作、翻译、生活建议等。  \n",
      "\n",
      "### **我的特点**：  \n",
      "🔹 **免费使用**：目前无需付费，随时为你提供帮助！  \n",
      "🔹 **超长上下文**：支持 **128K** 上下文记忆，能处理超长文档和复杂对话。  \n",
      "🔹 **多文档处理**：可以上传 **PDF、Word、Excel、PPT、TXT** 等文件，并帮你提取和分析内容。  \n",
      "🔹 **知识丰富**：我的知识截止到 **2024年7月**，能提供较新的信息。  \n",
      "🔹 **多语言支持**：可以用中文、英文等多种语言交流。  \n",
      "\n",
      "### **我能帮你做什么？**  \n",
      "📚 **学习**：解题思路、论文润色、知识讲解  \n",
      "💼 **工作**：简历优化、邮件撰写、数据分析  \n",
      "💻 **编程**：代码调试、算法讲解、技术文档解读  \n",
      "✍️ **写作**：创意写作、文案优化、故事构思  \n",
      "🌍 **生活**：旅行建议、健康小贴士、娱乐推荐  \n",
      "\n",
      "你可以随时向我提问，我会尽力提供准确、有用的答案！😊 有什么我可以帮你的吗？\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.utils import ConfigurableField\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.chat_models import QianfanChatEndpoint  # 使用QianfanChatEndpoint\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 初始化不同的模型\n",
    "ds_model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "gpt_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "# 配置可选模型\n",
    "model = gpt_model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"), \n",
    "    default_key=\"gpt\",  # 设置默认模型为GPT\n",
    "    deepseek=ds_model,\n",
    "    # 可以添加更多模型\n",
    ")\n",
    "\n",
    "# Prompt模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{query}\"),\n",
    "])\n",
    "\n",
    "# 构建Chain\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 运行时指定模型\n",
    "ret = chain.with_config(configurable={\"llm\": \"deepseek\"}).invoke(\"请自我介绍\")\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过LCEL，我们还可以实现更多高级功能：\n",
    "\n",
    "配置运行时变量\n",
    "故障回退\n",
    "并行调用\n",
    "逻辑分支\n",
    "动态创建Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结\n",
    "在本文中，我们介绍了LangChain的数据连接能力和流程编排功能。这些功能为构建复杂的大模型应用提供了强大支持：\n",
    "数据连接封装：提供了各种文档加载器、文本分割器和向量存储接口，方便处理各种格式的文档和实现语义搜索。\n",
    "LCEL：提供了一种声明式语言，简化了Chain的构建过程，支持各种复杂的应用模式，如RAG、工厂模式等。\n",
    "\n",
    "在下一篇文章中，我们将介绍LangChain的工作流框架LangGraph，它为构建更复杂的大模型应用提供了更高级的支持。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda12_0_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
